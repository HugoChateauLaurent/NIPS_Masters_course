{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "zwQS5X50Jbxf"
   },
   "source": [
    "In this assignment you will learn how to apply the REINFORCE algorithm within the OpenAI Gym environment. Make sure OpenAI gym is installed on your machine. Now let's import some relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOfznh3zJbxl"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers, logger\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Chain\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.optimizers import Adam\n",
    "from chainer import Variable\n",
    "import IPython\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yBPLN1LJbx3"
   },
   "source": [
    "We will make use of the classic CartPole environment provided by OpenAI Gym. Figure out what the details of this environment are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhQNKfgsJbx7"
   },
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v0'\n",
    "\n",
    "# You can set the level to logger.DEBUG or logger.WARN if you want to change the amount of output.\n",
    "logger.set_level(logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "env.reset()\n",
    "\n",
    "done = False\n",
    "for step in range(1000):\n",
    "    #env.render()\n",
    "    done = env.step(env.action_space.sample())[2]\n",
    "    if done:\n",
    "        env.reset()\n",
    "    step+=1\n",
    "    #time.sleep(0.01)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZQpuDSzJbyP"
   },
   "source": [
    "Let's define a baseline agent which just emits random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52A-L3gIJbyS"
   },
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0KU2XkgJbyh"
   },
   "source": [
    "Let's run the agent on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0wisW21Jbym",
    "outputId": "a6cf1cc1-d65e-4f7e-9adc-cbcff509922d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 522/1000 [00:00<00:00, 5218.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4948.36it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "env.seed(0)\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "episode_count = 1000\n",
    "done = False\n",
    "reward = 0\n",
    "\n",
    "R0 = np.zeros(episode_count)\n",
    "for i in tqdm.trange(episode_count):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = agent.act(ob, reward, done)\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "\n",
    "        R0[i] += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oefBg7d1JbzI"
   },
   "source": [
    "Let's create the REINFORCE agent. We assume that the policy is computed using an MLP with a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WW1Ha3BKJbzU"
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    \"\"\"Multilayer perceptron\"\"\"\n",
    "\n",
    "    def __init__(self, n_output=1, n_hidden=5):\n",
    "        super(MLP, self).__init__(l1=L.Linear(None, n_hidden), l2=L.Linear(n_hidden, n_output))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_LHMgDJJbze"
   },
   "source": [
    "1: A skeleton for the REINFORCEAgent is given. Implement the compute_loss and compute_score functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9ovHVUGJbzf"
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    \"\"\"Agent trained using REINFORCE\"\"\"\n",
    "\n",
    "    def __init__(self, action_space, model, optimizer=Adam()):\n",
    "\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer.setup(self.model)\n",
    "\n",
    "        # monitor score and reward\n",
    "        self.rewards = []\n",
    "        self.scores = []\n",
    "\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "\n",
    "        # linear outputs reflecting the log action probabilities and the value\n",
    "        policy = self.model(Variable(np.atleast_2d(np.asarray(observation, 'float32'))))\n",
    "\n",
    "        # generate action according to policy\n",
    "        p = F.softmax(policy).data\n",
    "\n",
    "        # normalize p in case tiny floating precision problems occur\n",
    "        row_sums = np.sum(p, axis=1)\n",
    "        p /= row_sums[:, np.newaxis]\n",
    "        \n",
    "        action = np.asarray([np.random.choice(p.shape[1], None, True, p[0])])\n",
    "        #action = np.random.choice(p.data.shape[1], None, True, p.data[0])\n",
    "        \n",
    "\n",
    "        return action, policy\n",
    "\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for this episode based on computed scores and accumulated rewards\n",
    "        \"\"\"\n",
    "        \n",
    "        J = 0\n",
    "        T = len(self.rewards)\n",
    "        gamma = .9\n",
    "        \n",
    "        Q_hat = np.array([np.multiply(np.array([gamma**i for i in range(T-t)]), np.array(self.rewards[t:], dtype=np.float32)).sum() for t in range(T)], dtype=np.float32)\n",
    "        J = F.matmul(Q_hat, F.stack(self.scores))\n",
    "        J = F.mean(J)\n",
    "    \n",
    "        return J\n",
    "\n",
    "    def compute_score(self, action, policy):\n",
    "        \"\"\"\n",
    "        Computes score\n",
    "\n",
    "        Args:\n",
    "            action (int):\n",
    "            policy:\n",
    "\n",
    "        Returns:\n",
    "            score\n",
    "        \"\"\"\n",
    "        score = F.softmax_cross_entropy(policy, action)\n",
    "        score = score.data\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySzdtdXGJbzn"
   },
   "source": [
    "Now we run the REINFORCE agent on the CartPole environment. Note that we update the agent after each episode for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jFuX_XhJbzq",
    "outputId": "87d8fdd2-7b16-4be1-ef36-4f4ff536d8f4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:25<00:00, 41.81it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "env.seed(0)\n",
    "\n",
    "network = MLP(n_output=env.action_space.n, n_hidden=3)\n",
    "agent = REINFORCEAgent(env.action_space, network, optimizer=Adam())\n",
    "\n",
    "episode_count = 1000\n",
    "R = np.zeros(episode_count)\n",
    "weights = np.copy(agent.model.l2.W.data)\n",
    "\n",
    "losses = []\n",
    "\n",
    "with chainer.using_config('train', True):\n",
    "    \n",
    "    for i in tqdm.trange(episode_count):\n",
    "\n",
    "        ob = env.reset()\n",
    "        agent.rewards = []\n",
    "        agent.scores = []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            action, policy = agent.act(ob, reward, done)\n",
    "            #action = np.asarray([action])\n",
    "            ob, reward, done, _ = env.step(action[0])\n",
    "\n",
    "            # get reward associated with taking the previous action in the previous state\n",
    "            agent.rewards.append(reward)\n",
    "            R[i] += reward\n",
    "\n",
    "            # recompute score function: grad_theta log pi_theta (s_t, a_t) * v_t\n",
    "            agent.scores.append(agent.compute_score(action, policy))\n",
    "\n",
    "        # we learn at the end of each episode\n",
    "        \n",
    "        loss = agent.compute_loss()\n",
    "        losses.append(loss.data)\n",
    "        print('loss gradients', chainer.grad([loss], [agent.model.l2.W]))\n",
    "\n",
    "        agent.model.cleargrads()\n",
    "        loss.backward()\n",
    "        agent.optimizer.update()\n",
    "        \n",
    "        IPython.display.clear_output()\n",
    "        \n",
    "        #plt.plot(range(i+1), losses)\n",
    "        #plt.title('Delta weights')\n",
    "        #plt.show()\n",
    "        #weights_new = np.copy(agent.model.l2.W.data)\n",
    "        #print(weights-weights_new)\n",
    "        #plt.imshow(np.absolute(weights-weights_new), interpolation='None', cmap='gray')\n",
    "        #plt.colorbar()\n",
    "        #weights = weights_new\n",
    "        #plt.title('Delta weights')\n",
    "        #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9je0fcmFJbz6"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "act() missing 1 required positional argument: 'done'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-64e9ef151c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: act() missing 1 required positional argument: 'done'"
     ]
    }
   ],
   "source": [
    "# You may want to run a video of the trained agent performing in the environment using the env.render() function.\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action, policy = agent.act(ob, reward)\n",
    "\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "      \n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0fKGA1TJb0E"
   },
   "source": [
    "2: Plot the cumulative reward for both RandomAgent and REINFORCEAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(range(episode_count), R0, label=\"RandomAgent\")\n",
    "plt.plot(range(episode_count), R, label=\"REINFORCEAgent.\")\n",
    "\n",
    "plt.title('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.legend(['RandomAgent', 'REINFORCEAgent.'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "act() missing 1 required positional argument: 'done'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-122d354c60fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: act() missing 1 required positional argument: 'done'"
     ]
    }
   ],
   "source": [
    "# You may want to run a video of the trained agent performing in the environment using the env.render() function.\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action, policy = agent.act(ob, reward)\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        print(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        time.sleep(0.01)\n",
    "            \n",
    "        \n",
    "      \n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SOW-MKI49-2018-SEM1-V-assignment_6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
