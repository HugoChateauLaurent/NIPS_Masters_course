{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGc6Y5_eKaPD"
   },
   "source": [
    "**SOW-MKI49: Neural Information Processing Systems**  \n",
    "*Weeks 4 and 5: Assignment (100 points + 20 bonus points)  \n",
    "Author: Luca and Umut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBlzrHdoKzvY"
   },
   "outputs": [],
   "source": [
    "# Group number: 24\n",
    "# Franka Buytenhuijs, s4356845\n",
    "# Hugo Chateau-Laurent, s1023970\n",
    "# Maria Tsfasman, s1021505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gz0FZAwiJ-al"
   },
   "outputs": [],
   "source": [
    "from chainer import cuda, datasets, serializers, optimizers\n",
    "from chainer.dataset import DatasetMixin, concat_examples\n",
    "import chainer.iterators as iterators\n",
    "from chainer.iterators import MultithreadIterator\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "if gpu:\n",
    "    import cupy as np\n",
    "else:\n",
    "    import numpy as np    \n",
    "import tqdm\n",
    "import os\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckO0T0SLAFf7"
   },
   "source": [
    "This is the decoder class. It transforms latents (features) to observables (images). It corresponds to p(x | z) in the context of variational inference (and the slides), where x is observables and y is latents.\n",
    "\n",
    "Task: (10 points)\n",
    "\n",
    "- Implement the decoder class for a variational autoencoder. Note that the decoder should output the Gaussian distribution parameters (mean and variance per pixel) of images rather than images themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ew8VZCkl-FdE"
   },
   "outputs": [],
   "source": [
    "dropout = .2 # global dropout rate\n",
    "\n",
    "class Decoder(chainer.ChainList):\n",
    "    def __init__(self, n_hlay, n_z, n_h, n_x): # <= you might want to pass some architecture parameters (e.g., #i/o units, etc.) here\n",
    "        \n",
    "        # Parameters:\n",
    "        #  n_hlay: number of hidden layers\n",
    "        #  n_z: dimension of latent space\n",
    "        #  n_h: dimension of hidden layers (if n_hlay>0)\n",
    "        #  n_x: dimension of x_out (=dimension of x_in)\n",
    "        \n",
    "        links=()\n",
    "        \n",
    "        if(n_hlay>0):\n",
    "            links += (L.Linear(n_z, n_h),)\n",
    "            for _ in range(n_hlay-1):\n",
    "                links += (L.Linear(n_h, n_h),)\n",
    "            n_z = n_h\n",
    "        \n",
    "        links += (L.Linear(n_z, n_x),) # mean\n",
    "        links += (L.Linear(n_z, n_x),) # variance\n",
    "        \n",
    "        print('Decoder:', len(links))\n",
    "                \n",
    "        super(Decoder, self).__init__(*links)\n",
    "\n",
    "    def __call__(self, z):         \n",
    "        for layer in self[:-2]:\n",
    "            z = F.relu(layer(z))   \n",
    "            z = F.dropout(z, dropout)\n",
    "        mean = F.relu(self[-2](z))\n",
    "        var = F.relu(self[-1](z))        \n",
    "        \n",
    "        return mean, var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d36RjWO6AKqO"
   },
   "source": [
    "This is the encoder class. It transforms observables (images) to latents (features). It corresponds to q(z | x) in the context of variational inference (and the slides), where z is latents and x is observables.\n",
    "\n",
    "Task: (10 points)\n",
    "\n",
    "- Implement the encoder class for a variational autoencoder. Note that the encoder should output the Gaussian distribution parameters (mean and variance per feature) of features rather than features themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-5xcJAWAFIC"
   },
   "outputs": [],
   "source": [
    "class Encoder(chainer.ChainList):\n",
    "    def __init__(self, n_hlay, n_x, n_h, n_z): # <= you might want to pass some architecture parameters (e.g., #i/o units, etc.) here\n",
    "        \n",
    "        # Parameters:\n",
    "        #  n_hlay: number of hidden layers\n",
    "        #  n_x: dimension of x_out (=dimension of x_in)\n",
    "        #  n_h: dimension of hidden layers (if n_hlay>0)\n",
    "        #  n_z: dimension of latent space\n",
    "\n",
    "        links=()\n",
    "        \n",
    "        if(n_hlay>0):\n",
    "            links += (L.Linear(n_x, n_h),)\n",
    "            for _ in range(n_hlay-1):\n",
    "                links += (L.Linear(n_h, n_h),)\n",
    "            n_x = n_h\n",
    "        \n",
    "        links += (L.Linear(n_x, n_z),) # mean\n",
    "        links += (L.Linear(n_x, n_z),) # variance\n",
    "        \n",
    "        print('Encoder:', len(links))\n",
    "                \n",
    "        super(Encoder, self).__init__(*links)\n",
    "\n",
    "    def __call__(self, x):  \n",
    "        for layer in self[:-2]:\n",
    "            x = F.relu(layer(x))\n",
    "            x = F.dropout(x, dropout)\n",
    "        mean = F.relu(self[-2](x))\n",
    "        var = F.relu(self[-1](x))        \n",
    "        \n",
    "        return mean, var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ch2iPFbBWVu"
   },
   "source": [
    "This is the loss class. The loss of encoder and decoder of a variational autoencoder is the evidence lower bound as follows:\n",
    "\n",
    "$L = D_{KL}(q(z | x), p(z)) -  E_{z\\sim q}[log p(x | z)]$\n",
    "\n",
    "The first term above is the KL divergence between the approximate posterior (q) and the prior (p), which can be interpreted as a form of regularization. You can assume that the prior is unit Gaussian. It can be implemented with the F.gaussian_kl_divergence function in Chainer.\n",
    "\n",
    "The second term above is the Gaussian negative log likelihood. This is the term that fits the data, which is very similar to the usual loss functions that you use in deep learning. It can be implemented with the F.gaussian_nll function in Chainer.\n",
    "\n",
    "Task: \n",
    "\n",
    "- Implement the loss class. (10 points)\n",
    "\n",
    "As input, it gets the following arguments:\n",
    "\n",
    "mean_y => mean of the encoded features (output of the encoder)  \n",
    "ln_var_y => log variance of the encoded features (output of the encoder)  \n",
    "x => input images (mini batch)  \n",
    "mean_x => mean of the decoded images (output of the decoder)  \n",
    "ln_var_x => mean of the decoded images (output of the decoder)  \n",
    "\n",
    "As output, it gives the loss.\n",
    "\n",
    "- Explain why we use log variance instead of variance. (5 points)\n",
    "\n",
    "    <span style=\"color:pink\">Using log variance brings numerical stability to the network. For instance, instead of outputing precise values between 0 and 1, it can output values between -inf and 0. The gradients are then bigger. However, we found that reluing the outputs and using ln_var_z as the standard deviation for the gaussian sample improves the performance and training of our model.</span>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iN6gKfG9BWfC"
   },
   "outputs": [],
   "source": [
    "#Explain why we use log variance instead of variance.\n",
    "\n",
    "\n",
    "class Loss(object):\n",
    "    def __call__(self, mean_z, ln_var_z, x, mean_x, ln_var_x):\n",
    "        return F.gaussian_nll(x, mean_x, ln_var_x) + F.gaussian_kl_divergence(mean_z, ln_var_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuS4ynBqBYt9"
   },
   "source": [
    "This is the model class. It combines the encoder and the decoder.\n",
    "\n",
    "Task: (20 points)\n",
    "\n",
    "- Implement the reparameterziation trick for sampling latents. (10 points)\n",
    "- Explain why we need to use this trick. (10 points)\n",
    "\n",
    "    <span style=\"color:pink\">As drawing a sample from a normal distribution is a stochastic process, you cannot backpropagate through it. The trick is to separate the parameters you want to optimize from the random process.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTLyWlw2BY3H"
   },
   "outputs": [],
   "source": [
    "class Model(chainer.Chain):\n",
    "    def __init__(self, decoder, encoder):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.decoder = decoder\n",
    "            self.encoder = encoder\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ln_var_z, mean_z = self.encoder(x)\n",
    "                \n",
    "        z = np.random.normal()*ln_var_z+mean_z # Sample latents (z) from the Gaussian with parameters ln_var_z, mean_z by using the reparameterization trick\n",
    "        \n",
    "        ln_var_x, mean_x = self.decoder(z)\n",
    "\n",
    "        return mean_z, ln_var_z, x, mean_x, ln_var_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12AV4tiBHo0W"
   },
   "outputs": [],
   "source": [
    "class Mnist(DatasetMixin):\n",
    "    def __init__(self, start_proportion, end_proportion):\n",
    "        dataset = datasets.get_mnist(False)[0 if chainer.config.train else 1]\n",
    "        size = dataset.shape[0]\n",
    "        self.dataset = dataset[int(size*start_proportion):int(size*end_proportion),:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UYXDr2-HrrA"
   },
   "source": [
    "This is a helper class to use the Mnist dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tC5HdWquIw7B"
   },
   "source": [
    "Task: (50 points)\n",
    "\n",
    "- Train the above defined variational autoencoder on the Mnist dataset. You can refer to the earlier assignments to implement your training loop. (25 points)\n",
    "\n",
    "- How good are the samples? Randomy sample some digits and visualize them. (10 points)\n",
    "\n",
    "<span style=\"color:pink\"> The samples are not too good, but enough to work with and relatively easy to reconstruct. </span>\n",
    "\n",
    "- How good are the reconstructions? Draw an Mnist like digit, encode it, decode it and visualize the digits. How different is the reconstruction from the original. (10 points)\n",
    "\n",
    "<span style=\"color:pink\"> The reconstructions appeared to be very good, especially for a simple fully-connected network.</span>\n",
    "\n",
    "- Repeat the last task but by drawing something other than a digit (e.g., a face). How accuracte is the reconstructions? Explain the results. (5 points)\n",
    "\n",
    "<span style=\"color:pink\"> We decided to work with fashion_mnist dataset with clothings instead of digits. As you can see, the result is less remarkable than for digits. The thing is that it doesn't reconstruct the details. What could work better in this case is a CNN. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/MNIST_20_show.png)\n",
    "![title](img/MNIST_20_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Fashion_2_20_show.png)\n",
    "![title](img/Fashion_2_20_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.log = {'training':[], 'test':[]}   \n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self.model)\n",
    "        self.loss = Loss()\n",
    "        \n",
    "    def test(self, x, show=-1):\n",
    "        with chainer.using_config('train', False):\n",
    "            mean_z, ln_var_z, x, mean_x, ln_var_x = self.model(x)\n",
    "            if show > -1:\n",
    "                if gpu:\n",
    "                    toshow = np.asnumpy(mean_x.data[show]).reshape(28,-1)\n",
    "                #plt.show()\n",
    "                else:\n",
    "                    toshow = mean_x.data[show].reshape(28,-1)\n",
    "                plt.imshow(toshow, cmap='gray')\n",
    "            err = self.loss(mean_z, ln_var_z, x, mean_x, ln_var_x)   \n",
    "            return float(err.data)\n",
    "    \n",
    "    def train(self, x):\n",
    "        err = self.loss(*self.model(x)) # Forward propagation \n",
    "        self.model.cleargrads()\n",
    "        err.backward() # backpropagation\n",
    "        self.optimizer.update() \n",
    "        return float(err.data)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, directory):\n",
    "        self = cls()\n",
    "        self.log = np.load('{}/log.npy'.format(directory))\n",
    "        serializers.load_npz('{}/weights.npz'.format(directory), self.model)\n",
    "        serializers.load_npz('{}/optimizer.npz'.format(directory), self.optimizer)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save(self, directory):\n",
    "        np.save('{}/log.npy'.format(directory), self.log)\n",
    "        serializers.save_npz('{}/weights.npz'.format(directory), self.model)\n",
    "        serializers.save_npz('{}/optimizer.npz'.format(directory), self.optimizer)\n",
    "\n",
    "load = False\n",
    "# model from scratch\n",
    "encoder = Encoder(2,784,500,20) # (n_hlay, n_x, n_h, n_z)\n",
    "decoder = Decoder(2,20,500,784) # (n_hlay, n_z, n_h, n_x)\n",
    "vae = Model(decoder, encoder)\n",
    "\n",
    "if gpu:\n",
    "    vae.to_gpu()\n",
    "optimizer = Optimizer(vae)   \n",
    "\n",
    "if load:\n",
    "    optimizer.load(\"./Models/100\")\n",
    "if gpu:\n",
    "    optimizer.model.to_gpu() \n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "training_set = Mnist(0,.5)\n",
    "test_set = Mnist(.5,1)\n",
    "training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)\n",
    "test_iterator = iterators.SerialIterator(test_set , batch_size, False, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I34eP98AIV-j"
   },
   "outputs": [],
   "source": [
    "# Create new folder to avoid FileExistsError\n",
    "root = '.'\n",
    "i=-1\n",
    "path = '{}/Models'.format(root)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "while i==-1 or os.path.isdir(path):\n",
    "    i+=1\n",
    "    path = '{}/Models/Weights_{}'.format(root, i)\n",
    "os.makedirs('{}/Models/Weights_{}'.format(root, i))\n",
    "\n",
    "optimizer.log['training'] = []\n",
    "optimizer.log['test'] = []\n",
    "\n",
    "for epoch in tqdm.tnrange(epochs):\n",
    "    \n",
    "    training_iterator.reset()\n",
    "    test_iterator.reset()\n",
    "    \n",
    "    mean_loss = []\n",
    "    for j, batch in enumerate(training_iterator):\n",
    "        mean_loss.append(optimizer.train(np.array(batch)))\n",
    "    optimizer.log['training'] += [numpy.mean(mean_loss)]\n",
    "\n",
    "    mean_loss = []\n",
    "    for j, batch in enumerate(test_iterator):\n",
    "        mean_loss.append(optimizer.test(np.array(batch)))\n",
    "    optimizer.log['test'] += [numpy.mean(mean_loss)]\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    plt.figure(figsize=(18,6))\n",
    "    samples_toshow = 8\n",
    "    for i in range(samples_toshow):\n",
    "        plt.subplot(2,samples_toshow,1+i)\n",
    "        optimizer.test(np.array(batch), show=i)\n",
    "        plt.subplot(2,samples_toshow,1+samples_toshow+i)\n",
    "        plt.imshow(batch[i].reshape(28,-1), cmap='gray')\n",
    "    plt.title(\"Epoch \"+str(epoch+1))\n",
    "    plt.show()\n",
    "    \n",
    "    if False:\n",
    "        # store weights and loss\n",
    "        os.makedirs('{}/{}'.format(path, epoch))\n",
    "        optimizer.save('{}/{}'.format(path, epoch))\n",
    "    \n",
    "    print(optimizer.log['test'])\n",
    "    print(optimizer.log['training'])\n",
    "    \n",
    "    plt.plot(range(epoch+1), optimizer.log['test'], label='test')\n",
    "    plt.plot(range(epoch+1), optimizer.log['training'], label='training')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzCbq-7SKmBW"
   },
   "source": [
    "Bonus task: Try the same experiments on a different dataset. (20 bonus points)\n",
    "\n",
    "**Let's now experiment on Fashion MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_fashion(DatasetMixin):\n",
    "    def __init__(self, start_proportion, end_proportion):\n",
    "        dataset = datasets.get_fashion_mnist(False)[0 if chainer.config.train else 1]\n",
    "        size = dataset.shape[0]\n",
    "        self.dataset = dataset[int(size*start_proportion):int(size*end_proportion),:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "# model from scratch\n",
    "encoder = Encoder(2,784,500,20) # (n_hlay, n_x, n_h, n_z)\n",
    "decoder = Decoder(2,20,500,784) # (n_hlay, n_z, n_h, n_x)\n",
    "vae = Model(decoder, encoder)\n",
    "\n",
    "if gpu:\n",
    "    vae.to_gpu()\n",
    "optimizer = Optimizer(vae)   \n",
    "\n",
    "if load:\n",
    "    optimizer.load(\"./Models/100\")\n",
    "if gpu:\n",
    "    optimizer.model.to_gpu()\n",
    "    \n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "training_set = Mnist_fashion(0,.5)\n",
    "test_set = Mnist_fashion(.5,1)\n",
    "training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)\n",
    "test_iterator = iterators.SerialIterator(test_set , batch_size, False, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new folder to avoid FileExistsError\n",
    "root = '.'\n",
    "i=-1\n",
    "path = '{}/Models'.format(root)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "while i==-1 or os.path.isdir(path):\n",
    "    i+=1\n",
    "    path = '{}/Models/Weights_{}'.format(root, i)\n",
    "os.makedirs('{}/Models/Weights_{}'.format(root, i))\n",
    "\n",
    "optimizer.log['training'] = []\n",
    "optimizer.log['test'] = []\n",
    "\n",
    "for epoch in tqdm.tnrange(epochs):\n",
    "    \n",
    "    training_iterator.reset()\n",
    "    test_iterator.reset()\n",
    "    \n",
    "    mean_loss = []\n",
    "    for j, batch in enumerate(training_iterator):\n",
    "        mean_loss.append(optimizer.train(np.array(batch)))\n",
    "    optimizer.log['training'] += [numpy.mean(mean_loss)]\n",
    "\n",
    "    mean_loss = []\n",
    "    for j, batch in enumerate(test_iterator):\n",
    "        mean_loss.append(optimizer.test(np.array(batch)))\n",
    "    optimizer.log['test'] += [numpy.mean(mean_loss)]\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    plt.figure(figsize=(18,6))\n",
    "    samples_toshow = 8\n",
    "    for i in range(samples_toshow):\n",
    "        plt.subplot(2,samples_toshow,1+i)\n",
    "        optimizer.test(np.array(batch), show=i)\n",
    "        plt.subplot(2,samples_toshow,1+samples_toshow+i)\n",
    "        plt.imshow(batch[i].reshape(28,-1), cmap='gray')\n",
    "    plt.title(\"Epoch \"+str(epoch+1))\n",
    "    plt.show()\n",
    "    \n",
    "    if False:\n",
    "        # store weights and loss\n",
    "        os.makedirs('{}/{}'.format(path, epoch))\n",
    "        optimizer.save('{}/{}'.format(path, epoch))\n",
    "    \n",
    "    print(optimizer.log['test'])\n",
    "    print(optimizer.log['training'])\n",
    "    \n",
    "    plt.plot(range(epoch+1), optimizer.log['test'], label='test')\n",
    "    plt.plot(range(epoch+1), optimizer.log['training'], label='training')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "weeks_6_and_7",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
